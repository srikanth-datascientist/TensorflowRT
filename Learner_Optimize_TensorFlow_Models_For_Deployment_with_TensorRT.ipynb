{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Learner-Optimize-TensorFlow-Models-For-Deployment-with-TensorRT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikanth-datascientist/TensorflowRT/blob/main/Learner_Optimize_TensorFlow_Models_For_Deployment_with_TensorRT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb3TdMZAkVNq"
      },
      "source": [
        "<h2 align=center> Optimize TensorFlow Models For Deployment with TensorRT</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndo48LDU8bYN"
      },
      "source": [
        "In this project, you will learn how to use the TensorFlow integration for TensorRT (also known as TF-TRT) to increase inference performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7wRN23n8h1t"
      },
      "source": [
        "### Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xaYlOAL8h4E"
      },
      "source": [
        "By the time you complete this project, you will be able to:\n",
        "\n",
        "- Optimize Tensorflow models using TF-TRT\n",
        "- Use TF-TRT to optimize several deep learning models at FP32, FP16, and INT8 precision\n",
        "- Observe how tuning TF-TRT parameters affects performance and inference throughput\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjV-PT6R80cB"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nCA3ypd83hZ"
      },
      "source": [
        "In order to be successful with this project, it is assumed you are:\n",
        "\n",
        "- Competent in the Python programming language\n",
        "- Familiar with Deep Learning, and understand what **inference** is\n",
        "- Familiar with TensorFlow, and its Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UX6T_tb-BsP"
      },
      "source": [
        "### Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSJch1oq-E4r"
      },
      "source": [
        "This project/notebook consists of several Tasks.\n",
        "\n",
        "- **[Task 1]()**: Introduction to the Project.\n",
        "- **[Task 2]()**: Setup your TensorFlow and TensorRT Runtime\n",
        "- **[Task 3]()**: Load the Data and Pre-trained Model\n",
        "- **[Task 4]()**: Create Batched Input\n",
        "- **[Task 5]()**: Load TensorFlow SavedModel\n",
        "- **[Task 6]()**: Get Baseline for Prediction Throughput and Accuracy\n",
        "- **[Task 7]()**: Convert a TensorFlow saved model into a TF-TRT Float32 Optimized Graph\n",
        "- **[Task 8]()**: Benchmark TF-TRT Float32\n",
        "- **[Task 9]()**: Convert to TF-TRT Float16 and Benchmark\n",
        "- **[Task 10]()**: TF-TRT INT8 Model\n",
        "- **[Task 11]()**: Converting to TF-TRT INT8\n",
        "- **[Task 12]()**: Benchmark TF-TRT INT8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CyTRMQw_CvM"
      },
      "source": [
        "## Task 2: Setup your TensorFlow and TensorRT Runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHkBraSZbpAH"
      },
      "source": [
        "You will only be able to use the Colab Notebook after you save it to your Google Drive folder. Click on the File menu and select “Save a copy in Drive…\n",
        "\n",
        "![Copy to Drive](https://drive.google.com/uc?id=1CH3eDmuJL8WR0AP1r3UE6sOPuqq8_Wl7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cripFOYdZrMX"
      },
      "source": [
        "## Check GPU Availability\n",
        "\n",
        "Check if your Colab notebook is configured to use Graphical Processing Units (GPUs). If zero GPUs are available, check if the Colab notebook is configured to use GPUs (Menu > Runtime > Change Runtime Type).\n",
        "\n",
        "![Hardware Accelerator Settings](https://drive.google.com/uc?id=1qrihuuMtvzXJHiRV8M7RngbxFYipXKQx)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faUXkqXnhOmq"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG4IBNn-2PWY"
      },
      "source": [
        "### Install TensorFlow-GPU 2.0 and TensorRT Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9Sz1GhkVNz"
      },
      "source": [
        "%%bash\n",
        "wget -q https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "\n",
        "dpkg -i nvidia-machine-learning-repo-*.deb\n",
        "apt-get -qq update\n",
        "\n",
        "sudo apt-get -qq install libnvinfer5 #libnvinfer6=6.0.1-1+cuda10.1\n",
        "\n",
        "pip install -q tensorflow-gpu==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyW580EYkVOP"
      },
      "source": [
        "# check TensorRT version\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version: \", tf.version.VERSION)\n",
        "print(\"TensorRT version: \")\n",
        "!dpkg -l | grep nvinfer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U8b2394CZRu"
      },
      "source": [
        "A successfull TensorRT installation looks like:\n",
        "\n",
        "```\n",
        "TensorFlow version:  2.0.0\n",
        "TensorRT version: \n",
        "ii  libnvinfer5   5.1.5-1+cuda10.1   amd64        TensorRT runtime libraries\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiQdEhAZeFo"
      },
      "source": [
        "## Restart the Runtime\n",
        "\n",
        "**Note** \n",
        "After installing the required Python packages, you'll need to restart the Colab Runtime Engine (Menu > Runtime > Restart runtime...)\n",
        "\n",
        "![Restart of the Colab Runtime Engine](https://drive.google.com/uc?id=1xnjAy2sxIymKhydkqb0RKzgVK9rh3teH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWYufTjPCMgW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyzwxjlm37jx"
      },
      "source": [
        "# Re-run after Kernel restart\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRWUHfC4_RiL"
      },
      "source": [
        "## Task 3: Load the Data and Pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-R2iN4akVOi"
      },
      "source": [
        "### Data\n",
        "We download several random images for testing from the Internet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kb5fkgP1Ji3"
      },
      "source": [
        "!mkdir ./data\n",
        "!wget  -qO ./data/img0.JPG \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Golden_Retriever_Carlos_%2810581910556%29.jpg/640px-Golden_Retriever_Carlos_%2810581910556%29.jpg\"\n",
        "!wget  -qO ./data/img1.JPG \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Red-shouldered_Hawk_%28Buteo_lineatus%29_-_Blue_Cypress_Lake%2C_Florida.jpg/407px-Red-shouldered_Hawk_%28Buteo_lineatus%29_-_Blue_Cypress_Lake%2C_Florida.jpg\"\n",
        "!wget  -qO ./data/img2.JPG \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Ostrich_male_RWD.jpg/636px-Ostrich_male_RWD.jpg\"\n",
        "!wget  -qO ./data/img3.JPG \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Working_pay_phone%2C_Jensen_Beach%2C_Florida%2C_September_4%2C_2012_001.JPG/360px-Working_pay_phone%2C_Jensen_Beach%2C_Florida%2C_September_4%2C_2012_001.JPG\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeV4r2YTkVO1"
      },
      "source": [
        "### Model\n",
        "\n",
        "Thoughout this project, we will be using InceptionV3. Here we import the model from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRBOikEkVO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFKQPoLO_ikd"
      },
      "source": [
        "def show_predictions(model):\n",
        "  for i in range(4):\n",
        "    img_path = './data/img%d.JPG'%i\n",
        "    img = image.load_img(img_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "\n",
        "    preds = model.predict(x)\n",
        "    print('{} - Predicted: {}'.format(img_path, decode_predictions(preds, top=3)[0]))\n",
        "\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.imshow(img);\n",
        "    plt.axis('off');\n",
        "    plt.title(decode_predictions(preds, top=3)[0][0][1])\n",
        "show_predictions(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrL3FEcdkVPA"
      },
      "source": [
        "When we benchmark our optimized TF-TRT models, they will be saved TensorFlow (not Keras) models. In order to have a fair comparison, here we save our Keras model as a TensorFlow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxlUF3rlkVPH"
      },
      "source": [
        "# Save the entire model as a TensorFlow SavedModel.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBu2RKs6kVPP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBQwBvlNm-J8"
      },
      "source": [
        "## Task 4: Create Batched Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUL_wImaUvqk"
      },
      "source": [
        "Using **batch inference** to send many images to the GPU at once promotes parallel processing and improve throughput."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWrmqsdiU2ya"
      },
      "source": [
        "The `batch_input` helper function takes a batch_size, and returns a tensor with the preprocessed images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQYfWhWoiJWM"
      },
      "source": [
        "# Re-run after Kernel restart\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nInI5vR_jLht"
      },
      "source": [
        "# Re-run after Kernel restart\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN7chQ_fjUAp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhlx2_EKjUDV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL3enlCO_3Js"
      },
      "source": [
        "## Task 5: Load TensorFlow SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfyrHPBljo8e"
      },
      "source": [
        "# Re-run after Kernel restart\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLv2mbWnVIl3"
      },
      "source": [
        "Here we load a previously saved InceptionV3 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZED92Kj5ZE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8Bb4iQzMlG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "217cYuLmAe3t"
      },
      "source": [
        "## Task 6: Get Baseline for Prediction Throughput and Accuracy\n",
        "\n",
        "### Naive Inference with TensorFlow 2\n",
        "\n",
        "The following will serve as a baseline for prediction throughput and accuracy. Now we perform inference with the optimized graph, and after a warmup, time and calculate throughput."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XtKF0qEVhEa"
      },
      "source": [
        "The helper function `predict_and_benchmark_throughput` will use the passed in model to perform predictions on the passed in batched input over a number of runs. It measures and reports throughput, as well as time for ranges of runs.\n",
        "\n",
        "Due to GPU initialization operations, we do not want to profile against initial inference -- so we can set a number of warmup runs to perform prior to benchmarking.\n",
        "\n",
        "`predict_and_benchmark_throughput` returns the predictions for all images for all runs, after the warmup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFBV6hQR7N3z"
      },
      "source": [
        "# Re-run after Kernel restart\n",
        "\n",
        "def predict_and_benchmark_throughput(batched_input, infer, N_warmup_run=50, N_run=1000):\n",
        "\n",
        "  elapsed_time = []\n",
        "  all_preds = []\n",
        "  batch_size = batched_input.shape[0]\n",
        "\n",
        "  for i in range(N_warmup_run):\n",
        "    labeling = infer(batched_input)\n",
        "    preds = labeling['predictions'].numpy()\n",
        "\n",
        "  for i in range(N_run):\n",
        "    start_time = time.time()\n",
        "\n",
        "    labeling = infer(batched_input)\n",
        "\n",
        "    preds = labeling['predictions'].numpy()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "    \n",
        "    all_preds.append(preds)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print('Steps {}-{} average: {:4.1f}ms'.format(i, i+50, (elapsed_time[-50:].mean()) * 1000))\n",
        "\n",
        "  print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))\n",
        "  return all_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZOv3IjNlqEC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYzJ_T8BDQE"
      },
      "source": [
        "### Observe Accuracy\n",
        "\n",
        "**NOTE:** We are not so concerned in this project about the accuracy of our predictions per se, only that they remain consistent as we optimize our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbj-UEOxkVPs"
      },
      "source": [
        "# Re-run after Kernel restart\n",
        "def show_predictions(model):\n",
        "\n",
        "  img_path = './data/img0.JPG'  # golden_retriever\n",
        "  img = image.load_img(img_path, target_size=(299, 299))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  x = tf.constant(x)\n",
        "\n",
        "  labeling = model(x)\n",
        "  preds = labeling['predictions'].numpy()\n",
        "  \n",
        "  # decode the results into a list of tuples (class, description, probability)\n",
        "  # (one such list for each sample in the batch)\n",
        "  print('{} - Predicted: {}'.format(img_path, decode_predictions(preds, top=3)[0]))\n",
        "  plt.subplot(2,2,1)\n",
        "  plt.imshow(img);\n",
        "  plt.axis('off');\n",
        "  plt.title(decode_predictions(preds, top=3)[0][0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MNzqtTfnJd8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAsVpzOUBRis"
      },
      "source": [
        "Before going to the next Task, please execute the cell below to restart the kernel and clear GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfXD9rZfp0yk"
      },
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlgsHdHxCUvk"
      },
      "source": [
        "### Network Transformation\n",
        "\n",
        "TF-TRT performs several important transformations and optimizations to the neural network graph. First, layers with unused outputs are eliminated to avoid unnecessary computation. Next, where possible, convolution, bias, and ReLU layers are fused to form a single layer. \n",
        "\n",
        "**Figure (a)** shows a typical convolutional network before optimization and \n",
        "\n",
        "**Figure (c)** shows the result of this vertical layer fusion on the original network from Figure (a) (fused layers are labeled CBR). Layer fusion improves the efficiency of running TF-TRT networks on the GPU.\n",
        "\n",
        "**Figure (b)** Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective outputs, as Figure (b) shows.\n",
        "\n",
        "Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters, resulting in a single larger layer for higher computational efficiency. The example in Figure (b) shows the combination of 3 1×1 CBR layers from Figure (c) that take the same input into a single larger 1×1 CBR layer. Note that the output of this layer must be disaggregated to feed into the different subsequent layers from the original input graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCs_-DoAC0H8"
      },
      "source": [
        "*Source*: [Speed up TensorFlow Inference on GPUs with TensorRT](https://blog.tensorflow.org/2018/04/speed-up-tensorflow-inference-on-gpus-tensorRT.html)\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img width=\"700px\" src='https://2.bp.blogspot.com/-nc-poLV8CNc/XhOI1wfgGjI/AAAAAAAACQI/3FlNTSKKrqMyTzR5XC5RCNnVuUY5EGmhQCLcBGAsYHQ/s1600/fig2.png' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure (a): An example convolutional  model with multiple convolutional and activation layers before optimization</p>\n",
        "    <p style=\"text-align: center;color:gray\">Figure (c): Horizontal layer fusion</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q-dJyM8FFU4"
      },
      "source": [
        "[Source](https://www.slideshare.net/cfregly/advanced-spark-and-tensorflow-meetup-20170506-reduced-precision-fp16-int8-inference-on-convolutional-neural-networks-with-tensorrt-and-nvidia-pascal-from-chris-gottbrath-nvidia): Advanced Spark and TensorFlow Meetup 2017-05-06 Reduced Precision (FP16, INT8) Inference on Convolutional Neural Networks with TensorRT and NVIDIA Pascal from Chris Gottbrath, Nvidia \n",
        "\n",
        "<div align=\"center\">\n",
        "    <img width=\"600px\" src='https://image.slidesharecdn.com/tensorrt-for-spark-tensorflow-meetup-final-170408190710/95/advanced-spark-and-tensorflow-meetup-20170506-reduced-precision-fp16-int8-inference-on-convolutional-neural-networks-with-tensorrt-and-nvidia-pascal-from-chris-gottbrath-nvidia-45-638.jpg' />\n",
        "    <p style=\"text-align: center;color:gray\"> Figure (c). Horizontal layer fusion </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTOCEd0LGn9h"
      },
      "source": [
        "When optimizing a TensorFlow model, TF-TRT can optimize either a subgraph or the entire graph definition. This capability allows the optimization procedure to be applied to the graph where possible and skip the non-supported graph segments. As a result, if the existing model contains a non-supported layer or operation, TensorFlow can still optimize the graph. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QquNykPMGpGC"
      },
      "source": [
        "Please see the [TF-TRT User Guide](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops) for a full list of supported operators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCt6eSdWHkex"
      },
      "source": [
        "## TF-TRT Workflow:\n",
        "\n",
        "Below, you can see a typical workflow of TF-TRT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKclpZ-HkhX"
      },
      "source": [
        "[Source](https://medium.com/tensorflow/high-performance-inference-with-tensorrt-integration-c4d78795fbfe): High performance inference with TensorRT Integration\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img width=\"600px\" src='https://miro.medium.com/max/875/1*hD_4k9bTEXnjuLHcaoFQRQ.png' />\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img width=\"600px\" src='https://miro.medium.com/max/875/1*DwxO-QF6Bz-H4aurRBIrjw.png' />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko-ehFXGHkkR"
      },
      "source": [
        "We now turn to the syntax for this one additional Optimize with TF-TRT step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MzdPdwI3kX"
      },
      "source": [
        "## Graph Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuAG6OpUI96Z"
      },
      "source": [
        "To perform graph conversion, we use `TrtGraphConverterV2`, passing it the directory of a saved model, and any updates we wish to make to its conversion parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9gP_85cJFk9"
      },
      "source": [
        "```python\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "\n",
        "trt.TrtGraphConverterV2(\n",
        "    input_saved_model_dir=None,\n",
        "    conversion_params=TrtConversionParams(precision_mode='FP32',\n",
        "                                          max_batch_size=1\n",
        "                                          minimum_segment_size=3,\n",
        "                                          max_workspace_size_bytes=8000000000,\n",
        "                                          use_calibration=True,\n",
        "                                          maximum_cached_engines=1,\n",
        "                                          is_dynamic_op=True,\n",
        "                                          rewriter_config_template=None,\n",
        "                                         )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTWxXJ4OJNx1"
      },
      "source": [
        "### Conversion Parameters\n",
        "\n",
        "Here is additional information about the most frequently adjusted conversion parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-F34Eu7JUGj"
      },
      "source": [
        "* __precision_mode__: This parameter sets the precision mode; which can be one of FP32, FP16, or INT8. Precision lower than FP32, meaning FP16 and INT8, would improve the performance of inference. The FP16 mode uses Tensor Cores or half precision hardware instructions, if possible. The INT8 precision mode uses integer hardware instructions.\n",
        "\n",
        "* __max_batch_size__: This parameter is the maximum batch size for which TF-TRT will optimize. At runtime, a smaller batch size may be chosen, but, not a larger one.\n",
        "\n",
        "* __minimum_segment_size__: This parameter determines the minimum number of TensorFlow nodes in a TF-TRT engine, which means the TensorFlow subgraphs that have fewer nodes than this number will not be converted to TensorRT. Therefore, in general, smaller numbers such as 5 are preferred. This can also be used to change the minimum number of nodes in the optimized INT8 engines to change the final optimized graph to fine tune result accuracy.\n",
        "\n",
        "* __max_workspace_size_bytes__: TF-TRT operators often require temporary workspace. This parameter limits the maximum size that any layer in the network can use. If insufficient scratch is provided, it is possible that TF-TRT may not be able to find an implementation for a given layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBMk1N4hJnSY"
      },
      "source": [
        "# Task 7: Convert a TensorFlow saved model into a TF-TRT Float32 Graph\n",
        "\n",
        "Convert a TensorFlow saved model into a TF-TRT optimized graph using Float32 precision. We will use the optimized graph to make predictions and will benchmark its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi8cUVHYsNZ9"
      },
      "source": [
        "Only a few lines of code are needed to use TF-TRT\n",
        "```python\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "  precision_mode=trt.TrtPrecisionMode.<FP16 or FP32>\n",
        "  max_workspace_bytes=8000000000\n",
        ")\n",
        "\n",
        "converter = trt.TrtGraphConverterV2(\n",
        "  input_saved_model_dir=input_saved_model_dir,\n",
        "  conversion_params=conversion_params\n",
        ")\n",
        "\n",
        "converter.convert()\n",
        "\n",
        "converter.save(output_saved_model_dir)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA6kENM9qSta"
      },
      "source": [
        "# Re-run after Kernel restart\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnHBooCNq_RB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlue_3npkVQC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uxjqJaSMxiI"
      },
      "source": [
        "## Task 8: Benchmark TF-TRT Float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIQNwij7NagW"
      },
      "source": [
        "Load the optimized TF model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai6bxNcNszHc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5agK6Q-tRQo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vcdKpowNdYD"
      },
      "source": [
        "Perform inference with the optimized graph, and after a warmup, time and calculate throughput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlU_S2dtr4Jq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AcfJ_55r4Mk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2F8t6cPkVQS"
      },
      "source": [
        "# Task 9: Convert to TF-TRT Float16 and Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxM1UsmFXE7y"
      },
      "source": [
        "In this task, you will update the convert_to_trt_graph_and_save function you worked with in the last task to be able to also perform conversion for Float16 precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ia_AlSDkVQT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ1JABY9NTBm"
      },
      "source": [
        "Load the optimized TF model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDbB6DWn0kJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_gerN5j9l6U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Y7D9FANWnA"
      },
      "source": [
        "Perform inference with the optimized graph, and after a warmup, time and calculate throughput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERjWgYlB3Fub"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp8cocOt3Hs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20_X2lOXN2Dw"
      },
      "source": [
        "Before going to the next Task, please execute the cell below to restart the kernel and clear GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJXaQqRN7dp"
      },
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKSJ-oizkVQY"
      },
      "source": [
        "## Task 10: TF-TRT INT8 Model\n",
        "\n",
        "We will discuss how TF-TRT is able to optimize to use Int8 precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTM8JlDyOSQ5"
      },
      "source": [
        "## Benefits of Reduced Precision Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH5At1gmOSTz"
      },
      "source": [
        "Typically, model training is performed using 32-bit floating point mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16) as the neural network architecture only requires a feed-forward network.\n",
        "\n",
        "Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput.\n",
        "\n",
        "Furthermore, recent NVIDIA GPUs are capable of executing 8-bit integer 4-element vector dot product instructions to accelerate deep neural network inference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTi4weePOSWe"
      },
      "source": [
        "[Source](https://developer.nvidia.com/blog/int8-inference-autonomous-vehicles-tensorrt/): Fast INT8 Inference for Autonomous Vehicles with TensorRT 3\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src='https://developer.nvidia.com/blog/wp-content/uploads/2017/12/dp4a-updated.png' />\n",
        "    <p style=\"text-align: center;color:gray\"> Figure (d). The DP4A instruction: 4-element dot product with accumulation.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8dC8LZSOSZR"
      },
      "source": [
        "## Reduced Dynamic Range of INT8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2LbnLVWOScG"
      },
      "source": [
        "\"While this new instruction provides faster computation, there is a significant challenge in representing weights and activations of deep neural networks in this reduced INT8 format. As *Table 1* shows, the dynamic range and granularity of representable values for INT8 is significantly limited compared to FP32 or FP16.\" [Source](https://developer.nvidia.com/blog/int8-inference-autonomous-vehicles-tensorrt/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEBUi7k3OSe7"
      },
      "source": [
        "[Source](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf): 8-bit Inference with TensorRT\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src='https://kezunlin.me/images/posts/635233-20181119142909032-2033014099.png' />\n",
        "    <p style=\"text-align: center;color:gray\"> Table 1. Dynamic range of FP32, FP16 and INT8.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ha6vUZHOSiH"
      },
      "source": [
        "## TF-TRT INT8 Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDFPcmmeOSlq"
      },
      "source": [
        "You might be wondering how it is possible to take a model which operates in 32 bit floating point precision, where you can represent billions of different numbers, and reduce that to only 8 bit integers which can only represent 256 possible values.\n",
        "\n",
        "The main reason is that, typically in deep learning, the values of weights and activations lie in very small ranges. So if we design our precious 8 bits to only represent this specific small range, we can usually maintain good accuracy while reducing the rounding error.\n",
        "\n",
        "The main challenge is to find the correct dynamic range of the inputs. TF-TRT uses a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation. In the next Task, you will see how to perform this calibration process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1znUXFfmSUBx"
      },
      "source": [
        "[Source](https://blog.tensorflow.org/2019/06/high-performance-inference-with-TensorRT.html): High performance inference with TensorRT Integration \n",
        "\n",
        "<div align=\"center\">\n",
        "    <img width=\"600px\" src='https://2.bp.blogspot.com/-UiS1VGo7zwQ/XflUnggwnYI/AAAAAAAAB5g/yU5qOXVfWw0x8oYaKyRcd4AKsbBltU64ACLcBGAsYHQ/s1600/fig7.png' />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY2y21NlKArH"
      },
      "source": [
        "Further quantization details: [https://developer.nvidia.com/gtc/2019/video/S9431/video](https://developer.nvidia.com/gtc/2019/video/S9431/video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqdxt16rSrTy"
      },
      "source": [
        "### Calibration Dataset Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhf9lBhxSsAg"
      },
      "source": [
        "When preparing the calibration dataset, you should capture the expected distribution of data in typical inference scenarios. You need to make sure that the calibration dataset covers all the expected scenarios, for example, clear weather, rainy day, night scenes, etc. When examining your own dataset, you should create a separate calibration dataset. The calibration dataset shouldn’t overlap with the training, validation or test datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VxmZWzfTSJ-"
      },
      "source": [
        "## Task 11: Converting to TF-TRT INT8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyksnYmYXVn4"
      },
      "source": [
        "You will convert a TensorFlow saved model into a TF-TRT optimized graph using INT8 precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJA7YnOG3_iO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HME-RBoDTZmL"
      },
      "source": [
        "## Task 12: Benchmark TF-TRT INT8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGULkANvXbGM"
      },
      "source": [
        "You will use the optimized graph to make predictions and will benchmark its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AeIIhWY3_k3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajCuEzou6j0d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1B-a6kA3_nX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-iLoAgN3_pr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I13snJ9VkVQh"
      },
      "source": [
        "## Conclusion\n",
        "In this notebook, we have demonstrated the process of creating TF-TRT FP32, FP16 and INT8 inference models from an original Keras FP32 model, as well as verify their speed and accuracy. \n"
      ]
    }
  ]
}